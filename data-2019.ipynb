{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b893a8-4177-460b-8149-f36cd0137228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/pturn22/econ470/a0/pyenv/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/pturn22/econ470/a0/pyenv/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from functions import (load_month, load_month_sa, load_month_pen, mapd_clean_merge)\n",
    "!pip install openpyxl\n",
    "\n",
    "# Settings\n",
    "monthlist = [f\"{m:02d}\" for m in range(1, 3)]  # \"01\", \"02\"\n",
    "y = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152b7808-5222-4c63-8a52-56b503527f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2721705, 19)\n",
      "  contractid  planid                                 org_type  \\\n",
      "0      90091     NaN                         HCPP - 1833 Cost   \n",
      "1      E0654   801.0  Employer/Union Only Direct Contract PDP   \n",
      "2      E0654   801.0  Employer/Union Only Direct Contract PDP   \n",
      "3      E0654   801.0  Employer/Union Only Direct Contract PDP   \n",
      "4      E0654   801.0  Employer/Union Only Direct Contract PDP   \n",
      "\n",
      "                                 plan_type partd snp eghp  \\\n",
      "0                         HCPP - 1833 Cost    No  No   No   \n",
      "1  Employer/Union Only Direct Contract PDP   Yes  No  Yes   \n",
      "2  Employer/Union Only Direct Contract PDP   Yes  No  Yes   \n",
      "3  Employer/Union Only Direct Contract PDP   Yes  No  Yes   \n",
      "4  Employer/Union Only Direct Contract PDP   Yes  No  Yes   \n",
      "\n",
      "                                           org_name  \\\n",
      "0  UNITED MINE WORKERS OF AMERICA HLTH & RETIREMENT   \n",
      "1             IBT VOLUNTARY EMPLOYEE BENEFITS TRUST   \n",
      "2             IBT VOLUNTARY EMPLOYEE BENEFITS TRUST   \n",
      "3             IBT VOLUNTARY EMPLOYEE BENEFITS TRUST   \n",
      "4             IBT VOLUNTARY EMPLOYEE BENEFITS TRUST   \n",
      "\n",
      "                                  org_marketing_name  \\\n",
      "0  United Mine Workers of America Health & Retire...   \n",
      "1  TEAMStar Medicare Part D Prescription Drug Pro...   \n",
      "2  TEAMStar Medicare Part D Prescription Drug Pro...   \n",
      "3  TEAMStar Medicare Part D Prescription Drug Pro...   \n",
      "4  TEAMStar Medicare Part D Prescription Drug Pro...   \n",
      "\n",
      "                                           plan_name  \\\n",
      "0                                                NaN   \n",
      "1  IBT Voluntary Employee Benefits Trust (Employe...   \n",
      "2  IBT Voluntary Employee Benefits Trust (Employe...   \n",
      "3  IBT Voluntary Employee Benefits Trust (Employe...   \n",
      "4  IBT Voluntary Employee Benefits Trust (Employe...   \n",
      "\n",
      "                              parent_org       contract_date     ssa    fips  \\\n",
      "0       UMWA Health and Retirement Funds  02/01/1974 0:00:00     NaN     NaN   \n",
      "1  IBT Voluntary Employee Benefits Trust  01/01/2007 0:00:00  2195.0     NaN   \n",
      "2  IBT Voluntary Employee Benefits Trust  01/01/2007 0:00:00  2275.0     NaN   \n",
      "3  IBT Voluntary Employee Benefits Trust  01/01/2007 0:00:00  2198.0     NaN   \n",
      "4  IBT Voluntary Employee Benefits Trust  01/01/2007 0:00:00  1000.0  1001.0   \n",
      "\n",
      "  state   county  enrollment  month  year  \n",
      "0   NaN      NaN         NaN      1  2019  \n",
      "1   NaN      NaN         NaN      1  2019  \n",
      "2   NaN      NaN         NaN      1  2019  \n",
      "3   NaN      NaN         NaN      1  2019  \n",
      "4    AL  Autauga         NaN      1  2019  \n"
     ]
    }
   ],
   "source": [
    "test = load_month(\"01\", 2019)\n",
    "print(test.shape)\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb9fda4c-eaa8-4da8-9cd6-a8458032b134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2139336/1402607324.py:17: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  lambda x: x.ffill().bfill()\n",
      "/tmp/ipykernel_2139336/1402607324.py:58: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  plan_data_2019 = plan_data.groupby(['contractid', 'planid', 'fips', 'year']).apply(agg_plan_year).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan data shape: (2720310, 23)\n"
     ]
    }
   ],
   "source": [
    "# Plan (enrollment & contract) data \n",
    "# Load and combine monthly data\n",
    "plan_data = pd.concat([load_month(m, y) for m in monthlist], ignore_index=True)\n",
    "\n",
    "# Sort data\n",
    "plan_data = plan_data.sort_values(['contractid', 'planid', 'state', 'county', 'month'])\n",
    "\n",
    "# Fill fips by state/county groups\n",
    "plan_data['fips'] = plan_data.groupby(['state', 'county'])['fips'].transform(\n",
    "    lambda x: x.ffill().bfill()\n",
    ")\n",
    "\n",
    "# Fill plan-level attributes\n",
    "plan_cols = ['plan_type', 'partd', 'snp', 'eghp', 'plan_name']\n",
    "for col in plan_cols:\n",
    "    plan_data[col] = plan_data.groupby(['contractid', 'planid'])[col].transform(\n",
    "        lambda x: x.ffill().bfill()\n",
    "    )\n",
    "\n",
    "# Fill contract-level attributes\n",
    "contract_cols = ['org_type', 'org_name', 'org_marketing_name', 'parent_org']\n",
    "for col in contract_cols:\n",
    "    plan_data[col] = plan_data.groupby(['contractid'])[col].transform(\n",
    "        lambda x: x.ffill().bfill()\n",
    "    )\n",
    "\n",
    "# Sort for aggregation\n",
    "plan_data = plan_data.sort_values(['contractid', 'planid', 'fips', 'year', 'month'])\n",
    "\n",
    "# Aggregate to yearly level\n",
    "def agg_plan_year(group):\n",
    "    nonmiss = group['enrollment'].notna()\n",
    "    n = nonmiss.sum()\n",
    "    enroll_vals = group.loc[nonmiss, 'enrollment']\n",
    "    \n",
    "    return pd.Series({\n",
    "        'n_nonmiss': n,\n",
    "        'avg_enrollment': enroll_vals.mean() if n > 0 else np.nan,\n",
    "        'sd_enrollment': enroll_vals.std() if n > 1 else np.nan,\n",
    "        'min_enrollment': enroll_vals.min() if n > 0 else np.nan,\n",
    "        'max_enrollment': enroll_vals.max() if n > 0 else np.nan,\n",
    "        'first_enrollment': enroll_vals.iloc[0] if n > 0 else np.nan,\n",
    "        'last_enrollment': enroll_vals.iloc[-1] if n > 0 else np.nan,\n",
    "        'state': group['state'].iloc[-1],\n",
    "        'county': group['county'].iloc[-1],\n",
    "        'org_type': group['org_type'].iloc[-1],\n",
    "        'plan_type': group['plan_type'].iloc[-1],\n",
    "        'partd': group['partd'].iloc[-1],\n",
    "        'snp': group['snp'].iloc[-1],\n",
    "        'eghp': group['eghp'].iloc[-1],\n",
    "        'org_name': group['org_name'].iloc[-1],\n",
    "        'org_marketing_name': group['org_marketing_name'].iloc[-1],\n",
    "        'plan_name': group['plan_name'].iloc[-1],\n",
    "        'parent_org': group['parent_org'].iloc[-1],\n",
    "        'contract_date': group['contract_date'].iloc[-1]\n",
    "    })\n",
    "\n",
    "plan_data_2019 = plan_data.groupby(['contractid', 'planid', 'fips', 'year']).apply(agg_plan_year).reset_index()\n",
    "print(f\"Plan data shape: {plan_data_2019.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7843e8f9-3425-4603-a0b6-ac53ee04d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MA_Cnty_SA_2011_07.csv', 'MA_Cnty_SA_2020_07.csv', 'MA_Cnty_SA_2015_06.csv', 'MA_Cnty_SA_2022_03.csv', 'MA_Cnty_SA_2023_02.csv', 'MA_Cnty_SA_2023_01.csv', 'MA_Cnty_SA_2022_01.csv', 'Read_Me_MA_Cnty_SA_2021.txt', 'MA_Cnty_SA_2015_09.csv', 'MA_Cnty_SA_2024_09.csv', 'MA_Cnty_SA_2023_06.csv', 'MA_Cnty_SA_2015_10.csv', 'MA_Cnty_SA_2009_06.csv', 'MA_Cnty_SA_2010_02.csv', 'MA_Cnty_SA_2018_10.csv', 'MA_Cnty_SA_2021_11.csv', 'MA_Cnty_SA_2020_09.csv', 'Read_Me_MA_Cnty_SA_2018.txt', 'MA_Cnty_SA_2013_02.csv', 'MA_Cnty_SA_200805.csv', 'MA_Cnty_SA_2018_07.csv', 'MA_Cnty_SA_2018_09.csv', 'MA_Cnty_SA_2021_09.csv', 'MA_Cnty_SA_2020_08.csv', 'MA_Cnty_SA_2020_10.csv', 'Read_Me_MA_Cnty_SA_2023.txt', 'Read_Me_MA_Cnty_SA_2024.txt', 'MA_Cnty_SA_2016_01.csv', 'MA_Cnty_SA_2009_12.csv', 'MA_Cnty_SA_2011_10.csv', 'MA_Cnty_SA_2016_07.csv', 'MA_Cnty_SA_2013_07.csv', 'MA_Cnty_SA_2024_08.csv', 'MA_Cnty_SA_2017_02.csv', 'MA_Cnty_SA_2014_02.csv', 'MA_Cnty_SA_2019_03.csv', 'MA_Cnty_SA_2024_07.csv', 'MA_Cnty_SA_2020_06.csv', 'MA_Cnty_SA_200801.csv', 'MA_Cnty_SA_2013_11.csv', 'MA_Cnty_SA_200610.csv', 'MA_Cnty_SA_200704.csv', 'MA_Cnty_SA_200710.csv', 'MA_Cnty_SA_200812.csv', 'Read_Me_MA_Cnty_SA_2017.txt', 'MA_Cnty_SA_2021_04.csv', 'MA_Cnty_SA_2010_10.csv', 'MA_Cnty_SA_2013_06.csv', 'MA_Cnty_SA_2019_12.csv', 'MA_Cnty_SA_2022_10.csv', 'MA_Cnty_SA_200808.csv', 'MA_Cnty_SA_2012_09.csv', 'MA_Cnty_SA_2014_01.csv', 'MA_Cnty_SA_2012_01.csv', 'MA_Cnty_SA_2012_10.csv', 'MA_Cnty_SA_2014_10.csv', 'Read_Me_MA_Cnty_SA_2009.txt', 'MA_Cnty_SA_2021_06.csv', 'MA_Cnty_SA_2009_04.csv', 'MA_Cnty_SA_2017_06.csv', 'MA_Cnty_SA_2024_02.csv', 'MA_Cnty_SA_2016_03.csv', 'MA_Cnty_SA_200702.csv', 'MA_Cnty_SA_2014_07.csv', 'MA_Cnty_SA_2020_03.csv', 'MA_Cnty_SA_200706.csv', 'MA_Cnty_SA_2016_09.csv', 'MA_Cnty_SA_2019_04.csv', 'MA_Cnty_SA_2017_10.csv', 'MA_Cnty_SA_200612.csv', 'MA_Cnty_SA_200807.csv', 'MA_Cnty_SA_2010_03.csv', 'MA_Cnty_SA_2024_12.csv', 'MA_Cnty_SA_2021_05.csv', 'MA_Cnty_SA_2018_01.csv', 'MA_Cnty_SA_200707.csv', 'MA_Cnty_SA_2018_12.csv', 'MA_Cnty_SA_2021_12.csv', 'MA_Cnty_SA_2020_01.csv', 'MA_Cnty_SA_2012_12.csv', 'MA_Cnty_SA_2013_09.csv', 'MA_Cnty_SA_2019_08.csv', 'MA_Cnty_SA_2009_05.csv', 'MA_Cnty_SA_2009_11.csv', 'MA_Cnty_SA_2021_10.csv', 'Read_Me_MA_Cnty_SA_2019.txt', 'MA_Cnty_SA_2009_01.csv', 'MA_Cnty_SA_200705.csv', 'MA_Cnty_SA_2020_04.csv', 'MA_Cnty_SA_2016_05.csv', 'MA_Cnty_SA_200611.csv', 'MA_Cnty_SA_2012_03.csv', 'MA_Cnty_SA_2010_05.csv', 'MA_Cnty_SA_2016_02.csv', 'MA_Cnty_SA_2023_10.csv', 'MA_Cnty_SA_2022_04.csv', 'MA_Cnty_SA_2015_04.csv', 'MA_Cnty_SA_2023_11.csv', 'MA_Cnty_SA_2015_08.csv', 'MA_Cnty_SA_2010_09.csv', 'MA_Cnty_SA_2018_02.csv', 'MA_Cnty_SA_2011_05.csv', 'MA_Cnty_SA_2019_01.csv', 'MA_Cnty_SA_2012_04.csv', 'MA_Cnty_SA_200712.csv', 'MA_Cnty_SA_2009_02.csv', 'MA_Cnty_SA_2014_08.csv', 'MA_Cnty_SA_2016_04.csv', 'MA_Cnty_SA_2018_04.csv', 'MA_Cnty_SA_2011_08.csv', 'MA_Cnty_SA_2021_07.csv', 'MA_Cnty_SA_2024_06.csv', 'MA_Cnty_SA_2019_07.csv', 'MA_Cnty_SA_2024_05.csv', 'MA_Cnty_SA_2020_02.csv', 'MA_Cnty_SA_2015_05.csv', 'MA_Cnty_SA_2024_01.csv', 'MA_Cnty_SA_2012_07.csv', 'MA_Cnty_SA_2016_08.csv', 'MA_Cnty_SA_2018_11.csv', 'MA_Cnty_SA_2022_11.csv', 'MA_Cnty_SA_2014_11.csv', 'readme_ma_cnty_sa_2008.txt', 'MA_Cnty_SA_2010_07.csv', 'Read_Me_MA_Cnty_SA_2012.txt', 'MA_Cnty_SA_2020_05.csv', 'MA_Cnty_SA_2015_02.csv', 'MA_Cnty_SA_200709.csv', 'MA_Cnty_SA_2013_12.csv', 'MA_Cnty_SA_2021_03.csv', 'Read_Me_MA_Cnty_SA_2016.txt', 'MA_Cnty_SA_2014_03.csv', 'MA_Cnty_SA_2010_11.csv', 'MA_Cnty_SA_2010_04.csv', 'MA_Cnty_SA_2024_11.csv', 'MA_Cnty_SA_2015_03.csv', 'MA_Cnty_SA_2023_05.csv', 'MA_Cnty_SA_2022_07.csv', 'Read_Me_MA_Cnty_SA_2011.txt', 'MA_Cnty_SA_2010_01.csv', 'MA_Cnty_SA_2011_06.csv', 'MA_Cnty_SA_2021_01.csv', 'MA_Cnty_SA_2015_11.csv', 'MA_Cnty_SA_2012_02.csv', 'MA_Cnty_SA_2012_08.csv', 'MA_Cnty_SA_2022_08.csv', 'MA_Cnty_SA_2009_09.csv', 'MA_Cnty_SA_200802.csv', 'MA_Cnty_SA_200809.csv', 'MA_Cnty_SA_2014_05.csv', 'MA_Cnty_SA_2016_12.csv', 'MA_Cnty_SA_2013_10.csv', 'readme_ma_cnty_sa.txt', 'MA_Cnty_SA_2019_11.csv', 'MA_Cnty_SA_2024_03.csv', 'MA_Cnty_SA_2017_04.csv', 'MA_Cnty_SA_200811.csv', 'MA_Cnty_SA_200804.csv', 'MA_Cnty_SA_2019_09.csv', 'MA_Cnty_SA_2014_12.csv', 'MA_Cnty_SA_2013_04.csv', 'MA_Cnty_SA_2017_11.csv', 'MA_Cnty_SA_2018_03.csv', 'MA_Cnty_SA_2011_02.csv', 'MA_Cnty_SA_2012_05.csv', 'MA_Cnty_SA_200810.csv', 'MA_Cnty_SA_2013_03.csv', 'MA_Cnty_SA_2023_09.csv', 'MA_Cnty_SA_2011_03.csv', 'MA_Cnty_SA_2017_12.csv', 'MA_Cnty_SA_2013_05.csv', 'MA_Cnty_SA_2020_12.csv', 'MA_Cnty_SA_2014_09.csv', 'MA_Cnty_SA_2013_08.csv', 'MA_Cnty_SA_2015_12.csv', 'Read_Me_MA_Cnty_SA_2015.txt', 'MA_Cnty_SA_2011_04.csv', 'MA_Cnty_SA_2014_04.csv', 'MA_Cnty_SA_200711.csv', 'MA_Cnty_SA_2011_01.csv', 'Read_Me_MA_Cnty_SA_2014.txt', 'MA_Cnty_SA_2024_10.csv', 'MA_Cnty_SA_2017_09.csv', 'MA_Cnty_SA_2009_07.csv', 'MA_Cnty_SA_200701.csv', 'MA_Cnty_SA_2018_08.csv', 'MA_Cnty_SA_2015_01.csv', 'MA_Cnty_SA_2016_11.csv', 'MA_Cnty_SA_2021_02.csv', 'MA_Cnty_SA_2011_11.csv', 'MA_Cnty_SA_2019_10.csv', 'MA_Cnty_SA_2019_02.csv', 'MA_Cnty_SA_2017_08.csv', 'MA_Cnty_SA_2023_08.csv', 'MA_Cnty_SA_2023_07.csv', 'MA_Cnty_SA_2016_10.csv', 'MA_Cnty_SA_2022_02.csv', 'MA_Cnty_SA_2023_12.csv', 'MA_Cnty_SA_200806.csv', 'MA_Cnty_SA_2018_05.csv', 'MA_Cnty_SA_2017_01.csv', 'MA_Cnty_SA_2022_06.csv', 'MA_Cnty_SA_2011_09.csv', 'MA_Cnty_SA_2020_11.csv', 'MA_Cnty_SA_2017_05.csv', 'MA_Cnty_SA_2019_05.csv', 'MA_Cnty_SA_2009_03.csv', 'MA_Cnty_SA_2021_08.csv', 'Read_Me_MA_Cnty_SA_2013.txt', 'MA_Cnty_SA_2023_04.csv', 'MA_Cnty_SA_2010_08.csv', 'MA_Cnty_SA_2022_05.csv', 'MA_Cnty_SA_2009_10.csv', 'MA_Cnty_SA_2012_11.csv', 'MA_Cnty_SA_200803.csv', 'Read_Me_MA_Cnty_SA_2020.txt', 'MA_Cnty_SA_2010_06.csv', 'readme_ma_cnty_sa_2007.txt', 'MA_Cnty_SA_2009_08.csv', 'MA_Cnty_SA_2017_07.csv', 'MA_Cnty_SA_2022_12.csv', 'MA_Cnty_SA_2018_06.csv', 'MA_Cnty_SA_2014_06.csv', 'MA_Cnty_SA_2017_03.csv', 'MA_Cnty_SA_2024_04.csv', 'MA_Cnty_SA_200708.csv', 'MA_Cnty_SA_200703.csv', 'MA_Cnty_SA_2023_03.csv', 'MA_Cnty_SA_2011_12.csv', 'MA_Cnty_SA_2012_06.csv', 'Read_Me_MA_Cnty_SA_2010.txt', 'MA_Cnty_SA_2022_09.csv', 'MA_Cnty_SA_2015_07.csv', 'Read_Me_MA_Cnty_SA_2022.txt', 'MA_Cnty_SA_2016_06.csv', 'MA_Cnty_SA_2019_06.csv', 'MA_Cnty_SA_2013_01.csv']\n"
     ]
    }
   ],
   "source": [
    "folder = \"../ma-data/ma/service-area/Extracted Data/\"\n",
    "import os\n",
    "print(os.listdir(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8a1085-4b64-405d-b649-61c21de20b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pturn22/econ470/a0/work/hwk2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9b7b20-05de-4461-9c7c-946a64cd96ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found service area files:\n",
      "['../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_03.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_12.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_04.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_08.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_01.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_07.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_11.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_09.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_10.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_02.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_05.csv', '../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_2019_06.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sa_folder = \"../ma-data/ma/service-area/Extracted Data\"\n",
    "files = [f for f in os.listdir(sa_folder) if f.startswith(f\"MA_Cnty_SA_{y}_\") and f.endswith(\".csv\")]\n",
    "files = [os.path.join(sa_folder, f) for f in files]  # full paths\n",
    "print(\"Found service area files:\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b58d6c-0f44-44f1-adb3-46453e4b51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contract id', 'organization name', 'organization type', 'plan type', 'partial', 'eghp', 'ssa', 'fips', 'county', 'state', 'notes']\n"
     ]
    }
   ],
   "source": [
    "service_year = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(f\"../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_{y}_{m}.csv\")\n",
    "          .rename(columns=lambda x: x.strip().lower())  # lowercase & strip spaces\n",
    "        for m in monthlist\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(service_year.columns.tolist())  # check if 'contractid' is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc4a6ce-af8f-423b-b6b0-2f49990340b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after cleaning: ['contract_id', 'org_name', 'org_type', 'plan_type', 'partial', 'eghp', 'ssa', 'fips', 'county', 'state', 'notes', 'month', 'year']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2139336/3134832345.py:56: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  lambda x: x.ffill().bfill()\n",
      "/tmp/ipykernel_2139336/3134832345.py:56: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  lambda x: x.ffill().bfill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service area data shape: (378670, 12)\n"
     ]
    }
   ],
   "source": [
    "# Service Data\n",
    "# Set working directory (optional, just to be sure)\n",
    "os.getcwd()\n",
    "\n",
    "# Year and months\n",
    "y = 2019\n",
    "monthlist = [f\"{i:02d}\" for i in range(1, 13)]  # '01' to '12'\n",
    "\n",
    "# Function to clean column names for consistency\n",
    "def clean_service_columns(df):\n",
    "    # Strip spaces, lowercase, replace spaces with underscores\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Rename known columns to standard names\n",
    "    df = df.rename(columns={\n",
    "        'contractid': 'contract_id',\n",
    "        'contract_id': 'contract_id',\n",
    "        'organization_name': 'org_name',\n",
    "        'organization_type': 'org_type',\n",
    "        'plan_type': 'plan_type'\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Load all monthly service area CSVs\n",
    "service_dfs = []\n",
    "for m in monthlist:\n",
    "    path = f\"../ma-data/ma/service-area/Extracted Data/MA_Cnty_SA_{y}_{m}.csv\"\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, dtype=str, encoding=\"latin-1\")\n",
    "        df = clean_service_columns(df)\n",
    "        df['month'] = int(m)\n",
    "        df['year'] = y\n",
    "        service_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: file not found: {path}\")\n",
    "\n",
    "# Combine all months\n",
    "service_year = pd.concat(service_dfs, ignore_index=True)\n",
    "\n",
    "# Check columns\n",
    "print(\"Columns after cleaning:\", list(service_year.columns))\n",
    "\n",
    "# Sort for stable fills\n",
    "service_year = service_year.sort_values(['contract_id', 'fips', 'state', 'county', 'month'])\n",
    "\n",
    "# Fill fips by state/county groups\n",
    "service_year['fips'] = service_year.groupby(['state', 'county'])['fips'].transform(\n",
    "    lambda x: x.ffill().bfill()\n",
    ")\n",
    "\n",
    "# Fill contract-level attributes\n",
    "contract_cols_sa = ['plan_type', 'partial', 'eghp', 'org_type', 'org_name']\n",
    "for col in contract_cols_sa:\n",
    "    if col in service_year.columns:\n",
    "        service_year[col] = service_year.groupby(['contract_id'])[col].transform(\n",
    "            lambda x: x.ffill().bfill()\n",
    "        )\n",
    "\n",
    "# Collapse to yearly: one row per contract x county (fips) x year\n",
    "service_year = service_year.sort_values(['contract_id', 'fips', 'year', 'month'])\n",
    "\n",
    "service_data_2019 = service_year.groupby(['contract_id', 'fips', 'year']).agg({\n",
    "    'state': 'last',\n",
    "    'county': 'last',\n",
    "    'org_name': 'last',\n",
    "    'org_type': 'last',\n",
    "    'plan_type': 'last',\n",
    "    'partial': 'last',\n",
    "    'eghp': 'last',\n",
    "    'ssa': 'last',\n",
    "    'notes': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Service area data shape: {service_data_2019.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3876c65a-7627-4503-afa8-c825ceefc969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xlrd in /home/pturn22/.local/lib/python3.13/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a684e6-27fa-4bd7-85b5-dc6459af518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /home/pturn22/econ470/a0/pyenv/lib/python3.13/site-packages (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Landscape data shape: (55400, 11)\n"
     ]
    }
   ],
   "source": [
    "# Plan Characteristics (premium) Data\n",
    "\n",
    "!pip install xlrd\n",
    "y = 2019\n",
    "\n",
    "# --- MA landscape data (A to M) ---\n",
    "ma_path_a = \"../ma-data/ma/landscape/Extracted Data/2019LandscapeSource file MA_AtoM 10122018.csv\"\n",
    "ma_col_names = [\"state\", \"county\", \"org_name\", \"plan_name\", \"plan_type\", \"premium\", \"partd_deductible\",\n",
    "                \"drug_type\", \"gap_coverage\", \"drug_type_detail\", \"contractid\",\n",
    "                \"planid\", \"segmentid\", \"moop\", \"star_rating\"]\n",
    "\n",
    "ma_data_a = pd.read_csv(ma_path_a, skiprows=6, dtype=str, encoding=\"latin-1\")\n",
    "\n",
    "# --- MINIMAL FIX: truncate to match column names ---\n",
    "ma_data_a = ma_data_a.iloc[:, :len(ma_col_names)]\n",
    "ma_data_a.columns = ma_col_names[:len(ma_data_a.columns)]\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['premium', 'partd_deductible']:\n",
    "    ma_data_a[col] = ma_data_a[col].str.replace('-', '0')\n",
    "    ma_data_a[col] = pd.to_numeric(ma_data_a[col].str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "\n",
    "ma_data_a['planid'] = pd.to_numeric(ma_data_a['planid'], errors='coerce')\n",
    "ma_data_a['segmentid'] = pd.to_numeric(ma_data_a['segmentid'], errors='coerce')\n",
    "\n",
    "\n",
    "# --- MA landscape data (N to W) ---\n",
    "ma_path_b = \"../ma-data/ma/landscape/Extracted Data/2019LandscapeSource file MA_NtoW 10122018.csv\"\n",
    "ma_data_b = pd.read_csv(ma_path_b, skiprows=6, dtype=str, encoding=\"latin-1\")\n",
    "\n",
    "# --- MINIMAL FIX: truncate to match column names ---\n",
    "ma_data_b = ma_data_b.iloc[:, :len(ma_col_names)]\n",
    "ma_data_b.columns = ma_col_names[:len(ma_data_b.columns)]\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['premium', 'partd_deductible']:\n",
    "    ma_data_b[col] = ma_data_b[col].str.replace('-', '0')\n",
    "    ma_data_b[col] = pd.to_numeric(ma_data_b[col].str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "\n",
    "ma_data_b['planid'] = pd.to_numeric(ma_data_b['planid'], errors='coerce')\n",
    "ma_data_b['segmentid'] = pd.to_numeric(ma_data_b['segmentid'], errors='coerce')\n",
    "\n",
    "# Combine A-M and N-W\n",
    "ma_data = pd.concat([ma_data_a, ma_data_b], ignore_index=True)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# MA-PD landscape data (Part D)\n",
    "# ================================\n",
    "\n",
    "mapd_path = \"../ma-data/ma/landscape/Extracted Data/PartCD/2019/Medicare Part D 2019 Plan Report 10012018.xls\"\n",
    "mapd_col_names = [\"state\", \"county\", \"org_name\", \"plan_name\", \"contractid\", \"planid\", \"segmentid\",\n",
    "                  \"org_type\", \"plan_type\", \"snp\", \"snp_type\", \"benefit_type\", \"below_benchmark\",\n",
    "                  \"national_pdp\", \"premium_partc\",\n",
    "                  \"premium_partd_basic\", \"premium_partd_supp\", \"premium_partd_total\",\n",
    "                  \"partd_assist_full\", \"partd_assist_75\", \"partd_assist_50\", \"partd_assist_25\",\n",
    "                  \"partd_deductible\", \"deductible_exclusions\", \"increase_coverage_limit\",\n",
    "                  \"gap_coverage\", \"gap_coverage_type\"]\n",
    "\n",
    "# Read Excel sheets without forcing column names\n",
    "mapd_data_a = pd.read_excel(mapd_path, sheet_name=\"Alabama to Montana\", skiprows=4, nrows=15854)\n",
    "mapd_data_b = pd.read_excel(mapd_path, sheet_name=\"Nebraska to Wyoming\", skiprows=4, nrows=20300)\n",
    "\n",
    "# Rename columns to match your expected list (truncate if needed)\n",
    "mapd_data_a.columns = mapd_col_names[:len(mapd_data_a.columns)]\n",
    "mapd_data_b.columns = mapd_col_names[:len(mapd_data_b.columns)]\n",
    "\n",
    "# Combine sheets\n",
    "mapd_data = pd.concat([mapd_data_a, mapd_data_b], ignore_index=True)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Clean and merge landscape + Part D\n",
    "# ================================\n",
    "\n",
    "landscape_2019 = mapd_clean_merge(ma_data, mapd_data, y)\n",
    "print(f\"Landscape data shape: {landscape_2019.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426e4ba6-8d1a-4945-84e0-3b2dec525ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penetration data shape: (3218, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2139336/4294036685.py:63: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  pen_2019 = ma_penetration.groupby(['fips', 'state', 'county', 'year']).apply(agg_penetration).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Penetration Data\n",
    "# Load and combine monthly penetration data\n",
    "pen_base_path = \"../ma-data/ma/penetration/Extracted Data\"\n",
    "\n",
    "def load_month_pen(m, y, base_path):\n",
    "    path = f\"{base_path}/State_County_Penetration_MA_{y}_{m}.csv\"\n",
    "    df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "    \n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\n",
    "        'State Name': 'state',\n",
    "        'County Name': 'county',\n",
    "        'FIPS': 'fips',\n",
    "        'SSA': 'ssa',\n",
    "        'Eligibles': 'eligibles',\n",
    "        'Enrolled': 'enrolled'\n",
    "    })\n",
    "    \n",
    "    # Remove commas and convert to numeric\n",
    "    df['eligibles'] = pd.to_numeric(df['eligibles'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "    df['enrolled'] = pd.to_numeric(df['enrolled'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "    \n",
    "    df[\"month\"] = int(m)\n",
    "    df[\"year\"] = y\n",
    "    return df\n",
    "\n",
    "# Load all months\n",
    "ma_penetration = pd.concat(\n",
    "    [load_month_pen(m, y, pen_base_path) for m in monthlist],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Fill missing FIPS\n",
    "ma_penetration['fips'] = ma_penetration.groupby(['state', 'county'])['fips'].transform(\n",
    "    lambda x: x.ffill().bfill()\n",
    ")\n",
    "\n",
    "# Collapse to yearly\n",
    "def agg_penetration(group):\n",
    "    n_elig = group['eligibles'].notna().sum()\n",
    "    n_enrol = group['enrolled'].notna().sum()\n",
    "    elig_vals = group['eligibles'].dropna()\n",
    "    enrol_vals = group['enrolled'].dropna()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'n_elig': n_elig,\n",
    "        'n_enrol': n_enrol,\n",
    "        'avg_eligibles': elig_vals.mean() if n_elig > 0 else np.nan,\n",
    "        'sd_eligibles': elig_vals.std() if n_elig > 1 else np.nan,\n",
    "        'min_eligibles': elig_vals.min() if n_elig > 0 else np.nan,\n",
    "        'max_eligibles': elig_vals.max() if n_elig > 0 else np.nan,\n",
    "        'first_eligibles': elig_vals.iloc[0] if n_elig > 0 else np.nan,\n",
    "        'last_eligibles': elig_vals.iloc[-1] if n_elig > 0 else np.nan,\n",
    "        'avg_enrolled': enrol_vals.mean() if n_enrol > 0 else np.nan,\n",
    "        'sd_enrolled': enrol_vals.std() if n_enrol > 1 else np.nan,\n",
    "        'min_enrolled': enrol_vals.min() if n_enrol > 0 else np.nan,\n",
    "        'max_enrolled': enrol_vals.max() if n_enrol > 0 else np.nan,\n",
    "        'first_enrolled': enrol_vals.iloc[0] if n_enrol > 0 else np.nan,\n",
    "        'last_enrolled': enrol_vals.iloc[-1] if n_enrol > 0 else np.nan,\n",
    "        'ssa': group['ssa'].iloc[-1] if 'ssa' in group.columns else np.nan\n",
    "    })\n",
    "\n",
    "pen_2019 = ma_penetration.groupby(['fips', 'state', 'county', 'year']).apply(agg_penetration).reset_index()\n",
    "print(f\"Penetration data shape: {pen_2019.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c111784-8b7e-4851-802e-75f583457886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebate data shape: (3617, 13)\n"
     ]
    }
   ],
   "source": [
    "# Rebate Data\n",
    "\n",
    "# Part C rebate data\n",
    "ma_path_a = \"../ma-data/ma/cms-payment/2019/2019PartCPlanLevel.xlsx\"\n",
    "risk_rebate_a = pd.read_excel(\n",
    "    ma_path_a,\n",
    "    sheet_name=\"result.html\",\n",
    "    usecols=\"A:G\",\n",
    "    skiprows=3,\n",
    "    nrows=3620 - 4 + 1,\n",
    "    header=None,\n",
    "    names=[\"contractid\",\"planid\",\"contract_name\",\"plan_type\",\n",
    "           \"riskscore_partc\",\"payment_partc\",\"rebate_partc\"],\n",
    "    engine=\"openpyxl\"\n",
    ")\n",
    "\n",
    "\n",
    "# Clean Part C data\n",
    "for col in ['riskscore_partc', 'payment_partc', 'rebate_partc']:\n",
    "    risk_rebate_a[col] = pd.to_numeric(risk_rebate_a[col].astype(str).str.replace(r'[^\\d.-]', '', regex=True), errors='coerce')\n",
    "risk_rebate_a['planid'] = pd.to_numeric(risk_rebate_a['planid'], errors='coerce')\n",
    "risk_rebate_a['year'] = 2019\n",
    "\n",
    "# Part D rebate data\n",
    "ma_path_b = \"../ma-data/ma/cms-payment/2019/2019PartDPlans.xlsx\"\n",
    "risk_rebate_b = pd.read_excel(\n",
    "    ma_path_b,\n",
    "    sheet_name=\"result.html\",\n",
    "    usecols=\"A:H\",\n",
    "    skiprows=3,\n",
    "    nrows=4460 - 4 + 1,\n",
    "    header=None,\n",
    "    names=[\"contractid\",\"planid\",\"contract_name\",\"plan_type\",\n",
    "           \"directsubsidy_partd\",\"riskscore_partd\",\"reinsurance_partd\",\"costsharing_partd\"],\n",
    "    engine=\"openpyxl\"\n",
    ")\n",
    "# Clean Part D data\n",
    "for col in ['directsubsidy_partd', 'reinsurance_partd', 'costsharing_partd']:\n",
    "    risk_rebate_b[col] = pd.to_numeric(risk_rebate_b[col].astype(str).str.replace(r'[^\\d.-]', '', regex=True), errors='coerce')\n",
    "risk_rebate_b['payment_partd'] = (risk_rebate_b['directsubsidy_partd'] + \n",
    "                                   risk_rebate_b['reinsurance_partd'] + \n",
    "                                   risk_rebate_b['costsharing_partd'])\n",
    "risk_rebate_b['planid'] = pd.to_numeric(risk_rebate_b['planid'], errors='coerce')\n",
    "\n",
    "# Select relevant columns for Part D\n",
    "risk_rebate_b = risk_rebate_b[['contractid', 'planid', 'payment_partd',\n",
    "                               'directsubsidy_partd', 'reinsurance_partd', 'costsharing_partd',\n",
    "                               'riskscore_partd']]\n",
    "\n",
    "# Merge Part C and Part D\n",
    "rebate_2019= risk_rebate_a.merge(risk_rebate_b, on=['contractid', 'planid'], how='left')\n",
    "print(f\"Rebate data shape: {rebate_2019.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3360037-8fcb-4067-a125-c13b214bda98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFS costs data shape: (3224, 9)\n"
     ]
    }
   ],
   "source": [
    "# FFS costs data\n",
    "ffs_path = \"../ma-data/ffs-costs/Extracted Data/FFS2019/FFS19.xlsx\"\n",
    "ffs_col_names = [\"ssa\",\"state\",\"county_name\",\"parta_enroll\",\n",
    "                 \"parta_reimb\",\"parta_percap\",\"parta_reimb_unadj\",\n",
    "                 \"parta_percap_unadj\",\"parta_ime\",\"parta_dsh\",\n",
    "                 \"parta_gme\",\"partb_enroll\",\n",
    "                 \"partb_reimb\",\"partb_percap\"]\n",
    "\n",
    "ffs_data = pd.read_excel(\n",
    "    ffs_path,\n",
    "    skiprows=2,\n",
    "    names=ffs_col_names,\n",
    "    na_values=['*', '.'],\n",
    "    usecols=range(len(ffs_col_names))   # matches 14 columns\n",
    ")\n",
    "\n",
    "ffscosts_2019 = ffs_data[['ssa', 'state', 'county_name',\n",
    "                          'parta_enroll', 'parta_reimb',\n",
    "                          'partb_enroll', 'partb_reimb']].copy()\n",
    "\n",
    "ffscosts_2019['year'] = 2019\n",
    "ffscosts_2019['mean_risk'] = np.nan\n",
    "ffscosts_2019['ssa'] = pd.to_numeric(ffscosts_2019['ssa'], errors='coerce')\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['parta_enroll', 'parta_reimb', 'partb_enroll', 'partb_reimb', 'mean_risk']:\n",
    "    ffscosts_2019[col] = pd.to_numeric(\n",
    "        ffscosts_2019[col].astype(str).str.replace(r'[^\\d.-]', '', regex=True),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "print(f\"FFS costs data shape: {ffscosts_2019.shape}\")\n",
    "for col in [\"parta_enroll\", \"parta_reimb\", \"partb_enroll\", \"partb_reimb\"]:\n",
    "    ffscosts_2019[col] = pd.to_numeric(ffscosts_2019[col], errors=\"coerce\")\n",
    "\n",
    "# Compute avg_ffscost (matches the logic from the R code)\n",
    "ae = ffscosts_2019[\"parta_enroll\"].fillna(0)\n",
    "be = ffscosts_2019[\"partb_enroll\"].fillna(0)\n",
    "ar = ffscosts_2019[\"parta_reimb\"]\n",
    "br = ffscosts_2019[\"partb_reimb\"]\n",
    "\n",
    "ffscosts_2019[\"avg_ffscost\"] = np.where(\n",
    "    (ae == 0) & (be == 0), 0,\n",
    "    np.where(\n",
    "        (ae == 0) & (be > 0), br / be,\n",
    "        np.where(\n",
    "            (ae > 0) & (be == 0), ar / ae,\n",
    "            (ar / ae) + (br / be)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b9a2d8-c250-47e3-952b-f1a57bf6d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged data shape: (75621, 62)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 0: Clean fips columns safely ---\n",
    "# Convert fips to numeric, coerce errors, fill NaN with 0, then convert to int->str\n",
    "service_data_2019['fips'] = pd.to_numeric(service_data_2019['fips'], errors='coerce').fillna(0).astype(int).astype(str)\n",
    "plan_data_2019['fips'] = pd.to_numeric(plan_data_2019['fips'], errors='coerce').fillna(0).astype(int).astype(str)\n",
    "\n",
    "# ================================\n",
    "# --- Step 1: Merge plan data with service area ---\n",
    "ma_2019 = plan_data_2019.merge(\n",
    "    service_data_2019[['contract_id', 'fips']],\n",
    "    left_on=['contractid', 'fips'],\n",
    "    right_on=['contract_id', 'fips'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# --- Step 2: Filter out territories and certain plan types ---\n",
    "excluded_states = ['VI', 'PR', 'MP', 'GU', 'AS', '']\n",
    "ma_2019 = ma_2019[\n",
    "    (~ma_2019['state'].isin(excluded_states)) &\n",
    "    (ma_2019['snp'] == 'No') &\n",
    "    ((ma_2019['planid'] < 800) | (ma_2019['planid'] >= 900)) &\n",
    "    (ma_2019['planid'].notna()) &\n",
    "    (ma_2019['fips'].notna())\n",
    "]\n",
    "\n",
    "# ================================\n",
    "# --- Step 3: Prepare penetration data for join ---\n",
    "pen_2019_join = pen_2019.copy()\n",
    "pen_2019_join = pen_2019_join.rename(columns={'state': 'state_long', 'county': 'county_long'})\n",
    "pen_2019_join['state_long'] = pen_2019_join['state_long'].str.lower()\n",
    "\n",
    "# Keep only unique fips entries\n",
    "pen_2019_join['ncount'] = pen_2019_join.groupby('fips')['fips'].transform('count')\n",
    "pen_2019_join = pen_2019_join[pen_2019_join['ncount'] == 1].drop(columns=['ncount'])\n",
    "\n",
    "# Join penetration data\n",
    "pen_2019_join['fips'] = pd.to_numeric(pen_2019_join['fips'], errors='coerce').fillna(0).astype(int).astype(str)\n",
    "ma_2019 = ma_2019.merge(pen_2019_join, on='fips', how='left', suffixes=('', '_pen'))\n",
    "\n",
    "# ================================\n",
    "# --- Step 4: Create state name lookup ---\n",
    "# Use last non-null state_long for each state\n",
    "state_2019 = ma_2019[['state', 'state_long']].dropna(subset=['state_long'])\n",
    "state_2019 = state_2019.drop_duplicates(subset='state', keep='last')\n",
    "state_2019 = state_2019.rename(columns={'state_long': 'state_name'})\n",
    "\n",
    "# Join state names\n",
    "full_2019 = ma_2019.merge(state_2019, on='state', how='left')\n",
    "\n",
    "# ================================\n",
    "# --- Step 5: Prepare landscape data for join ---\n",
    "landscape_2019_join = landscape_2019.copy()\n",
    "landscape_2019_join['state'] = landscape_2019_join['state'].str.lower()\n",
    "\n",
    "full_2019 = full_2019.merge(\n",
    "    landscape_2019_join,\n",
    "    left_on=['contractid', 'planid', 'state_name', 'county'],\n",
    "    right_on=['contractid', 'planid', 'state', 'county'],\n",
    "    how='left',\n",
    "    suffixes=('', '_land')\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# --- Step 6: Join rebate data ---\n",
    "rebate_2019_join = rebate_2019.drop(columns=['contract_name', 'plan_type'], errors='ignore')\n",
    "full_2019 = full_2019.merge(\n",
    "    rebate_2019_join,\n",
    "    on=['contractid', 'planid'],\n",
    "    how='left',\n",
    "    suffixes=('', '_reb')\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# --- Step 7: Calculate basic_premium and bid safely ---\n",
    "def calc_basic_premium(row):\n",
    "    rebate = row.get('rebate_partc', 0) or 0\n",
    "    premium = row.get('premium', 0) or 0\n",
    "    premium_partc = row.get('premium_partc', 0) or 0\n",
    "    partd = row.get('partd', 'No')\n",
    "    \n",
    "    if rebate > 0:\n",
    "        return 0\n",
    "    elif partd == 'No' and premium > 0 and pd.isna(premium_partc):\n",
    "        return premium\n",
    "    else:\n",
    "        return premium_partc\n",
    "\n",
    "def calc_bid(row):\n",
    "    rebate = row.get('rebate_partc', 0) or 0\n",
    "    basic_premium = row.get('basic_premium', 0) or 0\n",
    "    payment_partc = row.get('payment_partc')\n",
    "    riskscore_partc = row.get('riskscore_partc')\n",
    "    \n",
    "    if pd.isna(payment_partc) or pd.isna(riskscore_partc) or riskscore_partc == 0:\n",
    "        return np.nan\n",
    "    elif rebate == 0 and basic_premium > 0:\n",
    "        return (payment_partc + basic_premium) / riskscore_partc\n",
    "    elif rebate > 0 or basic_premium == 0:\n",
    "        return payment_partc / riskscore_partc\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "full_2019['basic_premium'] = full_2019.apply(calc_basic_premium, axis=1)\n",
    "full_2019['basic_premium'] = pd.to_numeric(full_2019['basic_premium'], errors='coerce')\n",
    "full_2019['bid'] = full_2019.apply(calc_bid, axis=1)\n",
    "\n",
    "print(f\"Final merged data shape: {full_2019.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d25ff2-e974-46e0-b12f-047f15b60332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data/output/data-2019.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "output_path = \"../data/output/data-2019.csv\"\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "full_2019.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0c811-97ee-4f68-86a9-35e01bc5a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
